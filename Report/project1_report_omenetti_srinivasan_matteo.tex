
%----------------------------------------------------------------------------------------
%	Machine Learning Assignment Template
%----------------------------------------------------------------------------------------

\documentclass[11pt]{scrartcl}
\newcommand*\student[1]{\newcommand{\thestudent}{{#1}}}

%----------------------------------------------------------------------------------------
%	INSERT HERE YOUR NAME
%----------------------------------------------------------------------------------------

\student{Omenetti Matteo, Ravi Srinivasan}

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\usepackage[utf8]{inputenc} % Required for inputting international characters
\usepackage[T1]{fontenc} % Use 8-bit encoding
\usepackage[sc]{mathpazo}
\usepackage{caption, subcaption}
\usepackage{hyperref}
\usepackage{inconsolata}

\usepackage[english]{babel} % English language hyphenation
\usepackage{amsmath, amsfonts} % Math packages
\usepackage{listings} % Code listings, with syntax highlighting
\usepackage{graphicx} % Required for inserting images
\graphicspath{{Figures/}{./}} % Specifies where to look for included images (trailing slash required)
\usepackage{float}
\usepackage{mathtools}
\usepackage{booktabs}


%----------------------------------------------------------------------------------------
%	DOCUMENT MARGINS
%----------------------------------------------------------------------------------------

\usepackage{geometry} % For page dimensions and margins
\geometry{
	paper=a4paper, 
	top=2.5cm, % Top margin
	bottom=3cm, % Bottom margin
	left=3cm, % Left margin
	right=3cm, % Right margin
}

%----------------------------------------------------------------------------------------
%	SECTION TITLES
%----------------------------------------------------------------------------------------

\usepackage{sectsty}
%\sectionfont{\vspace{6pt}\centering\normalfont\scshape}
\subsectionfont{\normalfont\bfseries} % \subsection{} styling
\subsubsectionfont{\normalfont\itshape} % \subsubsection{} styling
\paragraphfont{\normalfont\scshape} % \paragraph{} styling

%----------------------------------------------------------------------------------------
%	HEADERS AND FOOTERS
%----------------------------------------------------------------------------------------

\usepackage{scrlayer-scrpage}
\ofoot*{\pagemark} % Right footer
\ifoot*{\thestudent} % Left footer
\cfoot*{} % Centre footer

%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\title{	
	\normalfont\normalsize
	\textsc{Machine Learning for Health Care\\%
	ETH Zürich}\\
	\vspace{25pt}
	\rule{\linewidth}{0.5pt}\\
	\vspace{20pt}
	{\huge Project 1}\\
	\vspace{12pt}
	\rule{\linewidth}{1pt}\\
	\vspace{12pt}
}

\author{\LARGE \thestudent}

\date{\normalsize\today}

\begin{document}

\maketitle
\newpage
\tableofcontents
\newpage



\section{Introduction}
This report is part of the first project for the course "Machine Learning for Health Care" hosted at ETH Zürich. The scope of this project was to build different models for ECG classification for two different datasets PTB and MIT\_BIH.  All of the models described in this report can be found in  "src/" and they have been trained (in a reasonable amount of time) using the GPUs/TPUs offered by Google COLAB-Pro. All of the images that summarize the achitecture of the different models have been omitted due to the length constraints of the report, but they can be found in "./models\_architecture\_images"


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% VANILLA CNN %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Convolutional Neural Network (CNN)}
For the vanilla CNN, a smaller model compared to the one provided as a baseline was built. The reason behind this shallower architecture is that we were curious to understand whether the decreased training cost would also result in a statistically relevant decrease in performance. However, as it will be reported in the two subsections below this is not the case. 

%%%%%%% MIT %%%%%%%
\subsection{MIT\_BIH}
The code for this model can be found at "/src/MIT/CNN\_mit.ipynb".
The model on the Mit\_Bih dataset achieves a test accuracy score of 0.981, taking around 6 minutes to train. This model was trained for 33 epochs with the option to reduce the learning rate on plateau, before early stopping kicked in.
As image \ref{fig:cnn_mit_three} and the reported accuracy show the performance of this shallower model are very close to the baseline model (accuracy 0.985)  that instead takes more than 7 minutes to train on the same GPU.
\begin{figure}[htp]
\centering
\includegraphics[width=.3\textwidth]{../models_performance_graphs/mit/CNN_mit_train_validation.png}\hfill
\includegraphics[width=.3\textwidth]{../models_performance_graphs/mit/cnn_mit_train_val_acc.png}\hfill
\includegraphics[width=.3\textwidth]{../models_performance_graphs/mit/cnn_mit_confusion.png}
\caption{(left) train and validation loss of the vanilla CNN for the Mit\_bih dataset. \\ (center) train and validation accuracy of the vanilla CNN for the Mit\_bih dataset. \\(right) confusion matrix of the vanilla CNN for the Mit\_bih dataset}
\label{fig:cnn_mit_three}
\end{figure}

%%%%%%% PTB %%%%%%%
\subsection{PTB}
The code for this model can be found at "/src/PTB/CNN\_ptb.ipynb".
The model on the PTB dataset achieves a test accuracy score of 0.982, taking around 1 minute to train. 
As image \ref{fig:cnn_ptb_three} and the reported accuracy show, the performance of this shallower model is very close to the baseline model (accuracy 0.991).
The AURPRC and AUROC curves can be found in image \ref{fig:cnn_ptb_two}
\begin{figure}[htp]
\centering
\includegraphics[width=.30\textwidth]{../models_performance_graphs/ptb/cnn_ptb_accuracy.png}\hfill
\includegraphics[width=.30\textwidth]{../models_performance_graphs/ptb/cnn_ptb_loss.png}\hfill
\includegraphics[width=.30\textwidth]{../models_performance_graphs/ptb/cnn_ptb_confusion.png}\hfill
\caption{(left) train and validation loss of the vanilla CNN for the PTB dataset. \\ (center) train and validation accuracy of the vanilla CNN for the PTB dataset. \\(right) confusion matrix of the vanilla CNN for the PTB dataset}
\label{fig:cnn_ptb_three}
\end{figure}

\begin{figure}[htp]
\centering
\includegraphics[width=.50\textwidth]{../models_performance_graphs/ptb/cnn_ptb_auprc.png}\hfill
\includegraphics[width=.50\textwidth]{../models_performance_graphs/ptb/cnn_ptb_auroc.png}\hfill
\caption{(left) AURPC curve of the vanilla CNN for the PTB dataset. \\ (right) AUROC curve of the vanilla CNN for the PTB dataset}
\label{fig:cnn_ptb_two}
\end{figure}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% RES NET %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{CNN with Residual Blocks}
The code for this model can be found at "/src/MIT/ResNet\_mit.ipynb".
For this task, we designed a CNN architecture similar to ResNet (but shallower). This is the model that achieves the best performance. This model compared to the vanilla CNN adds residual blocks after each BatchNormalization layer.
%%%%%%% MIT %%%%%%%
\subsection{MIT\_BIH}
This model on the Mit\_Bih dataset, achives an accuracy of 0.988. The training curves and confusion matrix are summarized in image \ref{fig:res_mit_three}.
\begin{figure}[htp]
\centering
\includegraphics[width=.30\textwidth]{../models_performance_graphs/mit/res_net_mit_accuracy.png}\hfill
\includegraphics[width=.30\textwidth]{../models_performance_graphs/mit/res_net_mit_loss.png}\hfill
\includegraphics[width=.30\textwidth]{../models_performance_graphs/mit/res_net_mit_confusion.png}
\caption{(left) train and validation loss of the CNN with residual blocks for the Mit\_bih dataset. \\ (center) train and validation accuracy of the CNN with residual blocks for the Mit\_bih dataset. \\(right) confusion matrix of the CNN with residual blocks for the Mit\_bih dataset}
\label{fig:res_mit_three}
\end{figure}

%%%%%%% PTB %%%%%%%
\subsection{PTB}
The code for this model can be found at "/src/PTB/ResNet\_ptb.ipynb".
This model on the PTB dataset, achives an accuracy of 0.986. The training AUROC, AUPRC curves and confusion matrix  are summarized in images \ref{fig:res_ptb_three} and \ref{fig:res_ptb_two}
\begin{figure}[htp]
\centering
\includegraphics[width=.30\textwidth]{../models_performance_graphs/ptb/res_net_ptb_accuracy.png}\hfill
\includegraphics[width=.30\textwidth]{../models_performance_graphs/ptb/res_net_ptb_loss.png}\hfill
\includegraphics[width=.30\textwidth]{../models_performance_graphs/ptb/res_net_ptb_confusion.png}
\caption{(left) train and validation loss of the CNN with residual blocks for the PTB dataset. \\ (center) train and validation accuracy of the CNN with residual blocks for the PTB dataset. \\(right) confusion matrix of the CNN with residual blocks for the PTB dataset}
\label{fig:res_ptb_three}
\end{figure}

\begin{figure}[htp]
\centering
\includegraphics[width=.50\textwidth]{../models_performance_graphs/ptb/res_net_ptb_auprc.png}\hfill
\includegraphics[width=.50\textwidth]{../models_performance_graphs/ptb/res_net_ptb_auroc.png}\hfill
\caption{(left) AURPC curve of the CNN with residual blocks for the PTB dataset. \\ (right) AUROC curve of the CNN with residual blocks for the PTB dataset}
\label{fig:res_ptb_two}
\end{figure}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% TRANSFORMER %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Transformer}
This model consists of three transformers encoder blocks, stacked on top of each other with a standard CNN at the end. This model is very heavy to train. Without ColabPro the training for the Mit\_Bih dataset is around 16hours with the TPU.  Using ColabPro, the amount of time required to train the model decreases drastically. Overall, its performance is not great if compared to the other much lighter models, therefore we would not recommend using this model in these particular tasks. Moreover, this model is also significantly slower than all of the other models at prediction time.
%%%%%%% MIT %%%%%%%
\subsection{MIT\_BIH}
The code for this model can be found at "/src/MIT/Transformer\_mit.ipynb".
This model on the Mit\_Bih dataset, achives an accuracy of 0.976. The training curves and confusion matrix are summarized in image \ref{fig:transformer_mit_three}.
\begin{figure}[htp]
\centering
\includegraphics[width=.30\textwidth]{../models_performance_graphs/mit/transformer_mit_accuracy.png}\hfill
\includegraphics[width=.30\textwidth]{../models_performance_graphs/mit/transformer_mit_loss.png}\hfill
\includegraphics[width=.30\textwidth]{../models_performance_graphs/mit/transformer_mit_confusion.png}
\caption{(left) train and validation loss of the transformer for the Mit\_bih dataset. \\ (center) train and validation accuracy of the transformer for the Mit\_bih dataset. \\(right) confusion matrix of the transformer for the Mit\_bih dataset}
\label{fig:transformer_mit_three}
\end{figure}

%%%%%%% PTB %%%%%%%
\subsection{PTB}
The code for this model can be found at "/src/PTB/Transformer\_ptb.ipynb".
This model on the PTB dataset, achives an accuracy of 0.968. The training, AUROC, AUPRC curves and confusion matrix can  be found in images \ref{fig:tranformer_ptb_three} and \ref{fig:transformer_ptb_two}
\begin{figure}[htp]
\centering
\includegraphics[width=.30\textwidth]{../models_performance_graphs/ptb/transformer_ptb_accuracy.png}\hfill
\includegraphics[width=.30\textwidth]{../models_performance_graphs/ptb/transformer_ptb_loss.png}\hfill
\includegraphics[width=.30\textwidth]{../models_performance_graphs/ptb/transformer_ptb_confusion.png}
\caption{(left) train and validation loss of the transformer for the PTB dataset. \\ (center) train and validation accuracy of the transformer for the PTB dataset. \\(right) confusion matrix of the transformer for the PTB dataset}
\label{fig:tranformer_ptb_three}
\end{figure}

\begin{figure}[htp]
\centering
\includegraphics[width=.50\textwidth]{../models_performance_graphs/ptb/transformer_ptb_auprc.png}\hfill
\includegraphics[width=.50\textwidth]{../models_performance_graphs/ptb/transformer_ptb_auroc.png}\hfill
\caption{(left) AURPC curve of the transformer for the PTB dataset. \\ (right) AUROC curve of the transformer for the PTB dataset}
\label{fig:transformer_ptb_two}
\end{figure}








%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% ENSEMBLE %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Ensemble of models}
For this optional ensemble task, we decided to use two different approaches. For the first approach, we just summed up the different prediction probabilities of every model, and then we took the class with the highest one. For the second approach, we ran a logistic regression model on the output probabilities of every model, in order to better understand the relation between the models. 
The logistic regression ensemble achieves the best performance among all the models used for this assignment with an accuracy for the PTB dataset of 99.1\%. Once again ensembles of models of different kinds prove to be more robust than each single model taken separately.
The performance of each approach is summarized in their respective section.

%%%%%%% ENSEMBLE ONE %%%%%%%
\subsection{Average of the outputs}
%%%%%%% MIT %%%%%%%
\subsubsection{MIT\_BIH}
The code for this ensemble model can be found at "src/MIT/ensemble\_voting\_mit.ipynb".
This ensemble reaches an accuracy of 0.984. The confusion matrix can be found at \ref{fig:confusion_voting_mit}.
\begin{figure}[htp]
\centering
\includegraphics[width=.50\textwidth]{../models_performance_graphs/mit/ensemble_voting_confusion_mit.png}
\caption{Confusion matrix of the ensemble average of the outputs for the Mit\_Bih dataset}
\label{fig:confusion_voting_mit}
\end{figure}

%%%%%%% PTB %%%%%%%
\subsubsection{PTB}
The code for this ensemble model can be found at "src/PTB/ensemble\_voting\_ptb.ipynb".
This ensemble reaches an accuracy of 0.990. The AUROC, AUPRC curves and confusion matrix can be found in image \ref{fig:ensemble_voting_ptb_three}
\begin{figure}[htp]
\centering
\includegraphics[width=.30\textwidth]{../models_performance_graphs/ptb/ensemble_voting_AURPC_ptb.png}\hfill
\includegraphics[width=.30\textwidth]{../models_performance_graphs/ptb/ensemble_voting_AUROC_ptb.png}\hfill
\includegraphics[width=.30\textwidth]{../models_performance_graphs/ptb/ensemble_voting_confusion_ptb.png}
\caption{(left) AURPC curve of the ensemble average of the outputs for the PTB dataset. \\ (center) AUROC curve of the ensemble average of the outputs for the PTB dataset \\ (right) confusion matrix of the ensemble average of the outputs for the PTB dataset}
\label{fig:ensemble_voting_ptb_three}
\end{figure}

%%%%%%% ENSEMBLE TWO %%%%%%%
\subsection{Logistic regression on the outputs}

\subsubsection{MIT\_BIH}
The code for this model can be found at "src/MIT/ensemble\_logistic\_regression\_mit.ipynb".
This ensemble reaches an accuracy of 0.987. The confusion matrix can be found at \ref{fig:confusion_logistic_mit}.
\begin{figure}[htp]
\centering
\includegraphics[width=.50\textwidth]{../models_performance_graphs/mit/ensemble_logistic_confusion_mit.png}
\caption{Confusion matrix of the ensemble logistic regression for the Mit\_Bih dataset}
\label{fig:confusion_logistic_mit}
\end{figure}
%%%%%%% MIT %%%%%%%


%%%%%%% PTB %%%%%%%
\subsubsection{PTB}
The code for this ensemble model can be found at "src/PTB/ensemble\_logistic\_regression\_ptb.ipynb".
This ensemble reaches an accuracy of 0.991. The AUROC, AUPRC curves and confusion matrix can be found in image \ref{fig:ensemble_logistic_ptb_three}
\begin{figure}[htp]
\centering
\includegraphics[width=.30\textwidth]{../models_performance_graphs/ptb/ensemble_logistic_AURPC_ptb.png}\hfill
\includegraphics[width=.30\textwidth]{../models_performance_graphs/ptb/ensemble_logistic_AUROC_ptb.png}\hfill
\includegraphics[width=.30\textwidth]{../models_performance_graphs/ptb/ensemble_logistic_confusion_ptb.png}
\caption{(left) AURPC curve of the ensemble logistic regression for the PTB dataset. \\ (center) AUROC curve of the ensemble logistic regression for the PTB dataset \\ (right) confusion matrix of the ensemble logistic regression for the PTB dataset}
\label{fig:ensemble_logistic_ptb_three}
\end{figure}

\newpage
\section{Appendix}

\begin{table}[ht]
\centering
\caption{Summary of results}
\begin{tabular}[t]{lcc}
\toprule
&MIT\_BIH A&PTB\\
\midrule
Vanilla CNN &0.981&0.982\\
CNN Residual Blocks&0.988&0.986\\
Transformer&0.976&0.968\\
Ensemble Average&0.984&0.990\\
Ensemble Logistic&0.987&0.991\\
\bottomrule
\end{tabular}
\end{table}

\end{document}
